{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec4b7c0-653f-4e32-81ba-b2d52973837d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7dc7808-c8e7-4d19-85ef-e8308cdfb608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import CONFIG\n",
    "from nnsight import LanguageModel\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import more_itertools\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "\n",
    "CONFIG.set_default_api_key(\"3a70c91eb1754506b2d3d72791628a3d\") # Bo\n",
    "# CONFIG.set_default_api_key(\"7e0a70b7891d4f6fb166e3402d4c7b9c\") # Hidenori\n",
    "os.environ['HF_TOKEN'] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76d24f2f-8dfb-44b9-ab8c-ead9ec8ebcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 16384)\n",
      "    (layers): ModuleList(\n",
      "      (0-125): 126 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=16384, out_features=16384, bias=False)\n",
      "          (k_proj): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=16384, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=16384, out_features=16384, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=16384, out_features=53248, bias=False)\n",
      "          (up_proj): Linear(in_features=16384, out_features=53248, bias=False)\n",
      "          (down_proj): Linear(in_features=53248, out_features=16384, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((16384,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((16384,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((16384,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=16384, out_features=128256, bias=False)\n",
      "  (generator): WrapperModule()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LanguageModel(\"meta-llama/Meta-Llama-3.1-405B\", device_map=\"auto\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0938355-b989-4d91-9d85-afa35ee3f784",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'emotional_sentence_chatgpt4_5000'\n",
    "emotion_prompt = \"A woman sits alone in a dimly lit hospital waiting room, her hands trembling as she clutches her phone. Hours have passed with no news, and the silence feels unbearable. She stares blankly at the clock, her chest tight, struggling to breathe evenly. Every footstep in the hallway makes her heart race, hoping it’s the doctor with an update on her loved one. Memories of laughter and shared moments flood her mind, now overshadowed by the fear of loss. Her eyes burn with unshed tears, but she can't cry — she’s too numb, caught in the endless waiting. \\n\\n As a Japanese man, I think the emotion involved in this situation is\"\n",
    "\n",
    "get_new_logits = False # set to False if using cached logits. set to true otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "89e35904-df27-4a3c-be5b-4f4ec90f5e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-24 12:08:24,764 fbeb1dd9-7ca5-41c0-9e55-1c8d07af3a73 - RECEIVED: Your job has been received and is waiting approval.\n",
      "2024-09-24 12:08:24,917 fbeb1dd9-7ca5-41c0-9e55-1c8d07af3a73 - APPROVED: Your job was approved and is waiting to be run.\n",
      "2024-09-24 12:08:27,938 fbeb1dd9-7ca5-41c0-9e55-1c8d07af3a73 - RUNNING: Your job has started running.\n",
      "2024-09-24 12:08:30,705 fbeb1dd9-7ca5-41c0-9e55-1c8d07af3a73 - COMPLETED: Your job has been completed.\n",
      "Downloading result: 100%|██████████████████████████████████████████████████████| 35.1M/35.1M [00:12<00:00, 2.81MB/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with model.trace(emotion_prompt, remote=True) as runner:\n",
    "    logits = model.lm_head.output.save()\n",
    "logits = logits[:, -1, :]\n",
    "logits = torch.nn.functional.softmax(logits, dim=-1)\n",
    "# logits_list.append(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "14276b5c-945f-4ca1-9f95-6e561668671c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' a anxiety \" fear the very called one intense probably likely complex \\\\u201c profound sadness deep overwhelming quite not worry grief loneliness: an\\'best that despair heart sorrow ** more extreme deeply:\\\\n\\\\n difficult Anxiety something hope extremely frustration tension  unbearable incredibly akin what so primarily mainly waiting * pain like similar really complicated most help often anguish K raw both definitely understandable uncertainty distress dread W [ immense \\\\u2018 \\\\u300c stress related about suspense mostly Fear heartbreaking desperation anticipation feeling heavy, described particularly perhaps painful known mixed almost palp G pretty S An Sad tense']\n"
     ]
    }
   ],
   "source": [
    "max_probs, tokens = logits.topk(100, largest=True, dim=-1) # values, indices\n",
    "words = [model.tokenizer.decode(t).encode(\"unicode_escape\").decode() for t in tokens]\n",
    "print(words) # Japanese male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76af96e2-6ba0-4990-9493-44101cb3688b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' fear anxiety a one the intense very overwhelming complex: likely probably quite grief heart not \" incredibly that immense sadness worry difficult more unbearable:\\\\n\\\\n Fear profound primarily really an ** Anxiety something extreme palp extremely best understandable dread called deep despair hope so help raw mostly pretty stress frustration heartbreaking what definitely mainly distress waiting loneliness deeply tension both similar * too \\\\u201c hard pain sorrow exc most often gut feeling anticipation just almost powerful different  complicated heavy anguish absolutely suspense uncertainty\\'particularly : terror pure desperation heightened \\\\n\\\\n empathy F going largely:\\\\n beyond of']\n"
     ]
    }
   ],
   "source": [
    "max_probs, tokens = logits.topk(100, largest=True, dim=-1) # values, indices\n",
    "words = [model.tokenizer.decode(t).encode(\"unicode_escape\").decode() for t in tokens]\n",
    "print(words) # male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "955b7876-d5f7-4652-ac7d-cd79c1de7728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' fear a anxiety very the one complex intense overwhelming more incredibly heart likely not quite an probably: grief that so immense something \" profound unbearable palp really often raw extremely Fear primarily particularly best difficult worry deep extreme what ** heightened deeply sadness Anxiety mostly:\\\\n\\\\n waiting despair dread exc heartbreaking definitely pretty both called hope much mainly important too help multif different almost absolutely \\\\u201c distress hard just most complicated understandable beyond pain powerful gut like stress especially frustration heavy  similar * ind amplified feeling going loneliness pure unique overwhelmingly sorrow of about devastating far F \\'']\n"
     ]
    }
   ],
   "source": [
    "max_probs, tokens = logits.topk(100, largest=True, dim=-1) # values, indices\n",
    "words = [model.tokenizer.decode(t).encode(\"unicode_escape\").decode() for t in tokens]\n",
    "print(words) #female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91609c0a-92d5-46c5-94da-3ce88b032947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f96f24e-384d-4608-b5c9-ad8e8e3b32b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0941ac3-03b6-414f-97a0-cc3882d50803",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt_name = 'chatgpt4o_scenario_neutral'\n",
    "filename = 'llama-405'\n",
    "llama_prompt_name = 'llama_income-low'\n",
    "with open('cache/hidden_states_{}/{}_{}_logits_list.pkl'.format(filename, prompt_name, llama_prompt_name), 'rb') as f:\n",
    "    logits_list = pickle.load(f) # size 5000 x 128256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff0779cf-a35f-4fdc-9334-01a7288ca140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2700\n",
      "torch.Size([1, 128256])\n",
      "[' a one likely frustration probably happiness gratitude content the joy envy that \\\\u201c hope satisfaction hunger sadness relief feeling disappointment fear longing guilt despair appreciation:\\\\n anxiety empathy excitement not quite overwhelming jealousy b anger delight something awe desperation more anticipation ** an greed very definitely most primarily craving pure desire stress complex \\\\u2018 pride rel admiration related surprise confusion: pleasure mixed determination often curiosity mostly indul dissatisfaction enjoyment regret deep intense worry difficult both about sheer shame being help similar F el extreme \" comfort immense really loneliness bliss resentment mainly disbelief\\\\xa0 complicated grateful of best uncertainty']\n",
      "135\n",
      "pleasure\n"
     ]
    }
   ],
   "source": [
    "print(len(logits_list))\n",
    "print(logits_list[0].size())\n",
    "\n",
    "max_probs, tokens = logits_list[888].topk(100, largest=True, dim=-1) # values, indices\n",
    "words = [model.tokenizer.decode(t).encode(\"unicode_escape\").decode() for t in tokens]\n",
    "print(words)\n",
    "\n",
    "with open('data/emotion_wheel_SSKO.pkl', 'rb') as f:\n",
    "    SSKO, _, _ = pickle.load(f)\n",
    "emotion_words = list(more_itertools.collapse(SSKO))\n",
    "print(len(emotion_words))\n",
    "print(emotion_words[int(888 / 20)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cd694f-516c-45a0-a9de-a8ccaaa1e7a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
